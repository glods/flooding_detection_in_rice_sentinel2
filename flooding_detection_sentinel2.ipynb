{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Download and install packages"
      ],
      "metadata": {
        "id": "UeklDj28TxhE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVBEKKUuTFA5"
      },
      "outputs": [],
      "source": [
        "!pip install rasterio rasterstats fiona geopandas earthpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import rasterio as rio\n",
        "import numpy as np\n",
        "import re\n",
        "from datetime import datetime\n"
      ],
      "metadata": {
        "id": "pmHAJ9biT1qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def from_date_to_doy(date_string):\n",
        "    date = datetime.strptime(date_string, '%Y-%m-%d')\n",
        "    return date.timetuple().tm_yday\n",
        "  \n",
        "\n",
        "def flooding_module(data_path_mndwi, data_path_cloud):\n",
        "    '''\n",
        "    The objective of this project is to detect and map areas affected by flooding using satellite imagery. The function flooding_module() takes in two lists of image file names as \n",
        "    input, data_path_mndwi and data_path_cloud, which contain images of water bodies and cloud cover respectively. The function first sorts the lists of images by date in ascending\n",
        "     order, and then iterates over the images in the lists, processing them in chronological order from the oldest to the most recent.\n",
        "    For each pair of images, the function reads the data from the image files using the rasterio library and stores it in the variables im1 and im2. \n",
        "    It then applies a series of logical operations to the data to identify pixels that represent water bodies and that have low cloud cover. These pixels are then flagged in a \n",
        "    working array arrayimg14 with a value equal to the day of the year of the image. The final output of the function is an image of the same dimensions as the input images, \n",
        "    where each pixel is assigned a value that represents the day of the year when the pixel was last detected as a water body and had low cloud cover.\n",
        "    \n",
        "    Return\n",
        "    - Map\n",
        "    '''\n",
        "    # Sort data_path_mndwi and data_path_cloud by date in ascending order\n",
        "    data_path_mndwi = sorted(data_path_mndwi, key=lambda x: re.search(r'mndwi_(\\d{4}-\\d{2}-\\d{2})', x).group(1))\n",
        "    data_path_cloud = sorted(data_path_cloud, key=lambda x: re.search(r'cloud_(\\d{4}-\\d{2}-\\d{2})', x).group(1))\n",
        "    # Initialize the image\n",
        "    im = rio.open(data_path_mndwi[0], driver='Gtiff').read(1)\n",
        "    working_array = np.copy(im)\n",
        "    working_array[working_array > 0] = 1\n",
        "    arrayimg14 = working_array.copy()\n",
        "    q = 2\n",
        "    inc = 0\n",
        "    # Iterate over the list of images and cloud cover data\n",
        "    for mndwi_name, cloud_name in zip(data_path_mndwi, data_path_cloud):\n",
        "        mndwi_date = re.search(r'mndwi_(\\d{4}-\\d{2}-\\d{2})', mndwi_name).group(1)\n",
        "        cloud_date = re.search(r'cloud_(\\d{4}-\\d{2}-\\d{2})', cloud_name).group(1)\n",
        "        if mndwi_date == cloud_date:\n",
        "            im1 = rio.open(mndwi_name, driver='Gtiff').read(1)\n",
        "            im2 = rio.open(cloud_name, driver='Gtiff').read(1)\n",
        "            mask1 = np.logical_and.reduce((im1>0, im2<40)) \n",
        "            mask1 = np.logical_and.reduce((mask1, arrayimg14 ==1 ))\n",
        "            arrayimg14[mask1] = from_date_to_doy(mndwi_date)\n",
        "        q += 1\n",
        "        inc += 1\n",
        "    return arrayimg14\n"
      ],
      "metadata": {
        "id": "SOfG3DFwThJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create dataframes"
      ],
      "metadata": {
        "id": "XZ1qp6dAh2jN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_df(season, output_path, rgb_img, grid_path, area_name, operation, year, map_path):\n",
        "    \"\"\"\n",
        "    This function updates the dataframe of flooding areas and dates.\n",
        "    \n",
        "    Parameters:\n",
        "    - season (str): The season of the year.\n",
        "    - output_path (str): The path to save the output.\n",
        "    - rgb_img (str): The path to the RGB image.\n",
        "    - grid_path (str): The path to the grid shapefile.\n",
        "    - area_name (str): The name of the area.\n",
        "    - operation (str): The operation being performed (e.g. flooding).\n",
        "    - year (int): The year of the images.\n",
        "    - map_path (str): The path to the map of flooding areas.\n",
        "    \n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    def cumulative(df,year, savepath):\n",
        "        '''\n",
        "        This function calculates the cumulative area of flooding for a given year and saves the result as a csv and geojson file.\n",
        "        Inputs:\n",
        "        - df: dataframe containing the flooding data\n",
        "        - year: the year for which the cumulative flooding area is calculated\n",
        "        - savepath: the filepath where the result will be saved\n",
        "        Outputs:\n",
        "        - df: the modified dataframe with the cumulative flooding area added\n",
        "        '''\n",
        "        # Select the columns with the flooding data for the given year\n",
        "        columnsdate = [col for col in df.columns.to_list() if str(year) in col]\n",
        "        df = df.copy()\n",
        "        df = df.fillna(0)\n",
        "        # Loop through the columns to calculate the cumulative flooding area\n",
        "        for i in range(len(columnsdate)):\n",
        "            if i == 0:\n",
        "                df[columnsdate[i]] = df[columnsdate[i]]\n",
        "            else:\n",
        "                df[columnsdate[i]] = df[columnsdate[i - 1]] + df[columnsdate[i]]\n",
        "        # Save the result as a csv and geojson file\n",
        "        df.to_csv(savepath)\n",
        "        gdf1 = gpd.GeoDataFrame(df, geometry='geometry')\n",
        "        file = savepath.split('.')[0]\n",
        "        gdf1.to_file(file+'.geojson', driver = 'GeoJSON' )\n",
        "        return df\n",
        "\n",
        "    #---------------------------------------------------------------  \n",
        "    savepath = output_path + \"/dataframe/final_result\"\n",
        "    # Create a folder for cumulative data if there is not.\n",
        "    create_folder(savepath)\n",
        "    df = ''\n",
        "    gdfdata= gpd.read_file(grid_path)\n",
        "    if season == 'crossing_years': # Rainy season in Senegal ,We assume the rainy is from August to ...\n",
        "\n",
        "    # #   count_ref_img1 = zonal_stats(grid_path, output_path + '/rasters/' + operation+'map.tif' ,categorical=True )\n",
        "        count_ref_img1 = zonal_stats(grid_path, map_path ,categorical=True )\n",
        "        stat_df1 = pd.DataFrame(count_ref_img1)\n",
        "        coldoy = stat_df1.columns [1:]\n",
        "\n",
        "        coldate =  [from_doy_to_date(x, year) if x > 111 else from_doy_to_date(x + 256, year)  for x in coldoy  ]  #A year has a maximum of 366 days, so 366% 256 = 110\n",
        "     \n",
        "        stat_df1 = stat_df1.rename( columns = dict(zip(coldoy ,  coldate)) )\n",
        "        coldate.sort()\n",
        "        stat_df1 =  stat_df1[coldate] * 100*0.0001\n",
        "        df = stat_df1 \n",
        "        df[gdfdata.columns] = gdfdata\n",
        "      \n",
        "    else : \n",
        "        count_ref_img1 = zonal_stats(grid_path, map_path ,categorical=True )\n",
        "        stat_df1 = pd.DataFrame(count_ref_img1)\n",
        "        coldoy = stat_df1.columns [1:]\n",
        "\n",
        "        coldate =  [from_doy_to_date(x, year)  for x in coldoy  ]  #A year has a maximum of 366 days, so 366% 256 = 110\n",
        "\n",
        "      \n",
        "        stat_df1 = stat_df1.rename( columns = dict(zip(coldoy ,  coldate)) )\n",
        "        coldate.sort()\n",
        "        stat_df1 =  stat_df1[coldate] * 100*0.0001\n",
        "      \n",
        "        df = stat_df1\n",
        "        df[gdfdata.columns] = gdfdata\n",
        "      \n",
        "  \n",
        "\n",
        "    #-------------------------------------------------------------------------------------\n",
        "    # df =  stat_df1  \n",
        "    # rgb_img = '/content/drive/MyDrive/earthengine/NEWTEST/rgb_2021-01-02.tif'\n",
        " \n",
        "    # Path where the proportion will be saved\n",
        "    proportion_path = output_path + \"/dataframe/flooding_proportion.geojson\"\n",
        "    #-----------\n",
        "\n",
        "    gdfprop = create_proportion ( rgb_img, grid_path, proportion_path, year)\n",
        "\n",
        "    gdfprop [coldate] = df[coldate]\n",
        "    \n",
        "    for i  in coldate :\n",
        "          gdfprop [i] = gdfprop [ i ] / gdfprop ['aoi_area']\n",
        "    #+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
        "    #3.---------------------- For the cumulative ------------------------------\n",
        "\n",
        "    # savepath = output_path + \"/dataframe/cumulative\"\n",
        "    savepath = output_path + \"/dataframe/final_result\"\n",
        "    # Create a folder for cumulative data if there is not.\n",
        "    create_folder(savepath)\n",
        "    # # Cumulative of the initial flooding output\n",
        "\n",
        "\n",
        "    columnsdate = [col for col in df.columns.to_list() if str(year) in col]\n",
        "\n",
        "    df['aoi_area'] = gdfprop ['aoi_area']\n",
        "    \n",
        "    df ['total_'+operation+'_area'] =  df[columnsdate].sum(axis=1)\n",
        "    \n",
        "    \n",
        "    maxValueIndex1 = df[columnsdate].idxmax(axis=1)\n",
        "    df [operation+'_date'] = maxValueIndex1 \n",
        "    \n",
        "    gdfprop  ['total_'+operation+'_area'] =  df[columnsdate].sum(axis=1)\n",
        "    \n",
        "    gdfprop [operation+'_date'] = df [operation+'_date']\n",
        "    \n",
        "\n",
        "    col_ = [col for col in df.columns.to_list() if str(year) not in col] \n",
        "    \n",
        "    \n",
        "    colss = col_ + columnsdate #[1:]\n",
        "\n",
        "\n",
        "\n",
        "    df11 = cumulative (df[colss], year,savepath+'/' + operation + '_' + area_name + str(year)+  '.csv')\n",
        "\n",
        "    # df11 = cumulative (df[colss], year,savepath+'/' + operation + '_'  + area_name + str(year)+  '.geojson' )\n",
        "\n",
        "    # # Cumulative of the proportion\n",
        "    df22 = cumulative (gdfprop [colss], year, savepath+'/' + operation + '_proportion_' + area_name + str(year)+  '.csv')\n"
      ],
      "metadata": {
        "id": "RwTKIMBHh480"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}